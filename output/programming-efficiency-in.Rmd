---
title: "Programming Efficiency in R"
author: "Sebastian Marin"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: lumen
    toc: yes

---

```{r, message=F, warning=F, include = F}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #comment out when knitting the notebook

pacman::p_load(
  tidyverse, psych, DT, doParallel, data.table, rbenchmark, benchmarkme, microbenchmark, vroom, disk.frame, knitr
)

opts_chunk$set(echo=T, cache=F, prompt=F, tidy=T, comment=NA, message=F, warning=F)
``` 


<br />

<br />


# Benchmarking

There are several packages for benchmarking your machine and your code.

- [microbenchmark](https://cran.r-project.org/web/packages/microbenchmark/microbenchmark.pdf)
- [rbenchmark](https://cran.r-project.org/web/packages/rbenchmark/rbenchmark.pdf)
- [benchmarkme](https://cran.r-project.org/web/packages/benchmarkme/benchmarkme.pdf)
- and there's always base R's `system.time()`

## How fast is my machine? 

To maximize computational speed, first we have to know how long our code takes to run. 

Let's get an idea of local specs and compute speed. 

What version of R are you using? 
```{r}
# get major and minor version and paste together
paste0(version$major, ".", version$minor)
```

We should have the newest version of R (`r paste0(version$major, ".", version$minor)`). Having the latest version of R is one of the easiest ways to optimize code. New versions often provide speed boosts for commonly used functions. 

Having a faster computer might also make you more productive. To justify how you can save time using a faster computer, you will need some numbers. We can benchmark the performance of our computer using the [`benchmarkme` package](https://www.r-bloggers.com/2019/01/benchmarkme-new-version/). 
First, let's see what machine we're working with. 

```{r}
# benchmarkme  should already be loaded in your global environment
paste("RAM:", disk.frame::df_ram_size(), "GB") #how much RAM are you working with?
paste("CPUs:", benchmarkme::get_cpu()[3]) #how many logical processors do you have? 
```
You'll notice that we have more logical cores than physical cores at our disposal. This is due to hyperthreading. 

You can run a set of standardized benchmarks and compare your results to users who have uploaded their performance results. The `benchmark_io` function allows outputs the time it takes to read in data depending on its size (units = megabytes). Let's get performance for writing and reading a 50MB file and plot the results.
```{r}
write_read_bench <- benchmark_io( size = 5 ) #read and write benchmark
plot(write_read_bench) #plot results
upload_results(write_read_bench) #upload results for benchmark tracking
```

For general benchmarking tests, use `benchmark_std` to see how your machine deals with handling data. 
```{r}
std_bench <- benchmark_std() #benchmark matrix manipulation, calculation
plot(std_bench) #plot results
upload_results(std_bench) #upload results for benchmark tracking
```


## Benchmarking different read and write functions

There are different ways to write and read data in R. Let's create some data and test out read/write functions using: 

- `saveRDS()` and `readRDS()` functions in R
- `write.csv()` and `read.csv()` functions in the `utils` package
- `write_csv()` and `read_csv()` functions in the `readr` package
- `fwrite()` and `fread()` functions in the `data.table` package
- `vroom_write()` and `vroom()` functions in the `vroom` package

Let's write a function that creates some random data and assign them to a data frame object. 

```{r}
get_random_df <- 
  function(n_elements, ncol, min = 0, max = 1, seed = 42) {
    seed <- set.seed(seed)
    n <- n_elements #number of elements in a vector
    x <- runif(n, min = min, max = max) #random vector of real numbers
    x <- matrix(x, ncol=ncol)
    x <- as.data.frame(x)
    return(x)
  }

n <- 1e5
col <- 100
data <- get_random_df(n_elements = n, ncol = col)
```


Let's benchmark various functions for saving data frames using `rbenchmark::benchmark`. We will be saving as RDS and .csv files. 


```{r}
benchmark(
  rds = saveRDS(data, "../data/data.rds"),
  utils = write.csv(data, "../data/data_utils.csv"),
  readr = write_csv(data, "../data/data_readr.csv"),
  data.table = fwrite(data,"../data/data_dt.csv"), 
  vroom = vroom_write(data, "../data/data_vroom.csv"),
  
  replications = 1,
  columns = c('test', 'elapsed'),
  order = 'elapsed'
)
```

```{r}
benchmark(
  rds = readRDS("../data/data.rds"),
  utils = read.csv("../data/data_utils.csv"),
  readr = read_csv("../data/data_readr.csv"),
  data.table = fread("../data/data_dt.csv"), 
  vroom = vroom("../data/data_vroom.csv"),
  
  replications = 1,
  columns = c('test', 'elapsed'),
  order = 'elapsed'
)
```

Overall, if we want to use a .csv file format, `data.table::fread()` and `data.table::fwrite()` functions are the way to go. If we want to use a flexible and fast framework to store any R object (as opposed to just data frames), we should use `saveRDS()`. 

<br />

<br />


# Efficient programming
## Memory allocation 

Since R uses RAM automatically, we have to be careful with memory allocation. It's important to **vectorize** whenever possible. An efficient method is to instantiate empty vectors and then fill it with values later on.  A good tip is to never grow a vector. 

Below, we see different ways to creating a vector. (This example was taken from section 3.2.1 of [Efficient R Programming.](https://csgillespie.github.io/efficientR/)) [More information on for loops.](https://www.datacamp.com/community/tutorials/tutorial-on-loops-in-r)

```{r}
# this example was taken from section 3.2.1 of Efficient R Programming (2021)

n <- 1000

method1 = function(n) {
  vec = NULL #or vec = c()
  for (i in seq_len(n))
    vec = c(vec, i)
  vec
}

method2 = function(n) seq_len(n)
```


```{r}
benchmark(
  method1(n), method2(n),
  
  replications = 100,
  columns = c('test', 'elapsed'),
  order = 'elapsed'
)
```


## The *apply family

- `apply()` is generic: applies a function to a rows or columns of a matrix (i.e., to dimensions of an array)
  - should not be used for data frames since it tries to coerce to matrix first
- `lapply()` is a list apply which acts on a list or vector and returns a list.
- `sapply()` is a simple lapply (function defaults to returning a vector or matrix when possible)

`apply()` can be used for 2D arrays (i.e., matrix) and 3D arrays
```{r}
sum_row <- apply(data, 1, sum) #applies sum to rows
sum_col <- apply(data, 2, sum) #applies sum to rows
identical(sum_row, rowSums(data))
identical(sum_row, rowSums(data))
```

```{r}
array_3D <- array(seq(1e4), dim = c(4,4,2))
apply(array_3D, 1, sum) #apply sum across 2nd and 3rd dimension
apply(array_3D, c(1,2), sum) # apply sum across across 3rd dimension

```

`lapply()` is useful for lists. Say we want to apply a function per level of a factor variable. We can split the data by factor level, store the data as a list object, run the function, and combine the results again using `dplyr::bind_rows()`

```{r}
data$education <- 
  c("high_school","college", "masters", "phd") %>% #get a vector of edu level
  rep(., times = nrow(data)/4 ) %>% #repeat the vector for number of rows in the data divided by 4 since there's 4 levels
  sample(.) #takes a random sample of the vector, thereby randomizing it

data_ls <- split(data, data$education) #split the data into a list by edu level
#lapply(data_ls, sum) #warning if we do this because there's a character vector edu still in the data

data_ls <- lapply( data_ls, function(x) x[-length(x)] ) #since we know the position of the education variable is the max number of column, we can call length to get the number of columns and drop the last one
```

Let's see how lapply works in keeping the top 25% of cases based on a composite score.
```{r}
top_group <- function(x) {
  composite <- rowSums(x)
  bin <- quantile(composite)
  keep <- which(composite > bin["75%"])
  x <- x[keep, ]
  return(x)
}

data_ls_top <- lapply(data_ls, top_group)
data_top <- bind_rows(data_ls_top , .id = "education")
data_top %>% as_tibble

```

`sapply()` is 'simplified' apply that attempts to coerce the result to a multi-dimensional array, *if appropriate*. However, the function is not type consistent and can cause some problems. 
```{r}
df2 = data.frame(x = 1:5, y = letters[1:5])
df0 = data.frame()
sapply(df2, class) #character vector
sapply(df0, class) #list
df2[, 1:2]         #data frame
df2[, 1]           #numeric vector
```
[Learn more about the apply family here](https://stackoverflow.com/questions/3505701/grouping-functions-tapply-by-aggregate-and-the-apply-family)


<br />

<br />


# Tidyverse and data.table

Time spent processing your data at the beginning of projects can save time in the long run. We will walk through various ways to process data efficiently, both in terms of programming and algorithmic. For example, packages in `tidyverse` provide ways to efficiently print results and work with data. Specifically, the `dplyr` package provides a sufficiently quick yet easy way to process data, and the raw speed of `data.table` is useful for wrangling large data sets. Also, the essential `%>%` (pronounced "pipe") operator can clarify complex wrangling processes. 

## Tidyverse
One of the many benefits of `tidyverse` is that although this ecosystem of packages might be slower than base R at times, [it makes programming more intuitive/functional](https://tidyverse.tidyverse.org/articles/manifesto.html).

### `tibble()`
The tidyverse version of a `data.frame` object is a `tbl_df` from the `tibble` package. It is a class of data frame that makes printing results, subsetting data, and factoring more accessible. 

In printing a `tbl_df`, we can see the class of each variable; we do not when printing a `data.frame`. Also, `tbl_df` prints tidy output to the console, whereas the full `data.frame` object is printed. That is, printing  **tbl** is "tidyer" than **df**. 

```{r}
tbl <- tibble(
  num = 1:50, 
  char = rep(c("x","y"), each = 25)
) 
tbl 

df <- data.frame(
  num = 1:50, 
  char = rep(c("x","y"), each = 25)
) 
df 
```



Tibbles are strict about subsetting. If you try to access a variable that does not exist, youâ€™ll get an error.

```{r, error=T}
tbl$letter
```

Tibbles also delineate [ and [[: [ always returns another tibble/data.frame, [[ always returns a vector.
```{r}
identical(tbl$char, tbl[["char"]]) #two vectors
identical(tbl$char, tbl["char"]) #one vector, one tbl_df
```

### `pivot_longer()`

Another efficient way to wrangle data is `tidyr::pivot_longer()`. The function makes "wide" tables "long". We're going to make education variables binary and then gather the binaries into a single column. 

```{r}
df <- get_random_df(2000, 2, 1, 5) #get random data frame with 2000 elements and 2 columns with values from 1-5
make_binary <- function(x) sample( rep(c(1,0), each = 500) ) #function that gets vectors of randomly sampled zeros and ones for the number of rows in df
```


```{r}
df <- df %>% #assigning manipulated df to object "df" (same object)
  rename(score = V1, #renaming to "score"
         performance = V2) %>% #renaming to "performance"
  mutate(   #mutating df to include new vars 
    phd = make_binary(),  
    masters = make_binary(),
    college = make_binary(), 
    high_school = make_binary()
  )
df %>% as_tibble #printing df as a tbl_df
```


```{r}
df %>% #grab df
  pivot_longer(
    phd:high_school, #variables to gather  
    names_to = "highest_edu", #name of gathered column 
    values_to = "edu" #name of values column 
  ) -> df_long #assign to a new tbl_df object
df_long #print the tbl_df
```

Once we have a long table, we can easily group data for further analysis.
Let's group by the highest education attained and run a simple linear regression by the group variable. We are predicting performance based on some score. 
```{r}
df_long %>% #grab df_long
  group_by(highest_edu) %>% #group by highest education attained
  do(model = lm(performance ~ score, data = .)) %>% #model simple linear regressions by grouping variable -- do() applies expression to each group that can range in complexity
  summarize(broom::tidy(model)) #summarize models -- broom is a package that converts statistical objects into tibbles
```



### `separate()`
Another useful function is `separate`, which splits joint variables based on a common delimiter. For example, if a variable holds multiple pieces of information, we can separate the information based on some pattern. 

```{r}
sep <- tibble(college_grad = "University of Florida - University of Minnesota, Twin Cities")
sep

separate(sep, col = college_grad, into = c("college", "grad"), sep = "-")
separate(sep, col = college_grad, into = c("college", "grad"), sep = " - ")
```

### `dplyr`
The almighty `dplyr` is essential for working with data. `dplyr` is fast (C++ backend) and intuitive to type. It works well with tidy data, databases, and large data structures. In fact, we have used some `dplyr` functions already in this section. 

`dplyr` was designed to be used with the `%>%` operator in mind. Typically, each data processing step to be represented as a new line, which we have seen before. 
Below are main `dplyr` functions and with descriptions.

- `filter()`, `slice()`: subsets rows by attribute (filter) or position (slice)
- `arrange()`: orders data by variables 
- `select()`; subsets columns 
- `rename()`: rename columns (assigning new name = old name) 
- `distinct()`: returns unique, non-duplicated rows
- `mutate()`: creates new variables on existing data set 
- `summarize()`: summarizes variables by function and returns a new tbl_df
- `group_by()`: groups data defined by variables where functions are performed by group
- ... [and many more](https://dplyr.tidyverse.org/reference/index.html)!

```{r}
df %>% #grab df
  group_by(phd, masters) %>%  #group by phd, then within phd group by masters
  summarize(
    mean_score = mean(score), #get mean score for groups
    mean_performance = mean(performance) #get mean performance for groups
  ) %>% 
  arrange(phd, -masters) #arrange results in ascending  order for phd (the default) and descending order for masters within the levels of phd
```

### `across()`
`across()` is a relatively new and long-awaited function in `dplyr`. The function applies some other function across columns. The equivalent would be using `apply()` on margin 2 (i.e., columns). 

Let's coerce all numeric columns to characters using `dplyr()` and `apply()`, and compare the performance of each.

```{r}
benchmark(
  dplyr = df %>% mutate(across(everything(), as.character)), 
  apply = apply(df, 2, as.character),
  
  replications = 100, 
  columns = c("test","elapsed"),
  order = "elapsed"
)

```


## data.table

`data.table` is typically thought of as an alternative to `dplyr`. It values speed over intuitive and accessible programming. You can learn more at [datatable-intro](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html), [datatable-reshape]() and [datatable-reference-semantics](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reference-semantics.html). 


<br />

<br />


# Example: Big-Five Inventory data

## Simulating more BFI data

If we want to simulate data with a multivariate normal distribution, we're going to need means, SDs, and the covariance matrix. Let's use the BFI data set from the `psych` package. To learn more about the BFI data set, run `?psych::bfi`. 


```{r}
rm(list = ls()) #clear the global environment

data(bfi) #get data
glimpse(bfi) #take a look at the data structure using glimpse from the tibble package
bfi <- bfi[complete.cases(bfi),] #drop cases with at least 1 missing value 
bfi_trim <- bfi %>% select(-gender, -age, -education) #or subset(bfi, select= -c(gender, age, education)) to drop gender, age, edu columns 
glimpse(bfi_trim) #check the structure again to compare

mu <- colMeans(bfi_trim) #get variable means
sd <- apply(bfi_trim, 2, sd) #get variable SDs
cor_m <- cor(bfi_trim) #get correlation matrix
cov_m <- sd %*% t(sd) * cor_m #get covariance matrix
```

Simulating data can take awhile, especially at larger sample sizes. We can simulate multivariate normal data by using `MASS::mvrnorm()`. Let's also checkout another function called `anMC::mvrnormArma()` to compare speed. 

```{r}
set.seed(9201994)
benchmark(
  MASS::mvrnorm(1e6, mu, cov_m, empirical=T),
  anMC::mvrnormArma(1e6, mu, cov_m, 0),
  
  replications = 1,
  columns = c('test', 'elapsed'),
  order = 'elapsed'
) #benchmarking simulating 1mil cases of the sampled data set 
```

We can clearly see from the output above that `mvrnormArma()` is (~6x) faster. Why? Because it calls C++ on the backend to create the simulated data. 

Let's assign the new data to a data frame object to compare differences saving and loading data. Matrices are typically faster to work with, but IO psychologist typically work with tabular data, so we will be using data frames, tibbles, or data tables. Let's start with the most common: data frames. We should also add back in our categorical variables one vector at a time (at random).  

### BFI data
```{r}
set.seed(9201994)
bfi_df <-anMC::mvrnormArma(1e6, mu, cov_m, 0) %>% 
  t() %>% #transpose
  as.data.frame() #get data.frame
names(bfi_df) <- names(bfi_trim)
```

### Gender
```{r}
# let's keep things proportional 
gender <- round(table(bfi$gender)/nrow(bfi), digits = 5) #gender proportion
gender #view gender

n <- length(gender)
genders <- as.numeric(names(gender)) 
gender_vec <- c()

microbenchmark(times = 10, unit = "s",
               # growing a vector
               gender_vec_grow =
                 for(i in 1:n) {
                   gender_vec <- c(gender_vec, rep(genders[i], 1e6 * gender[i]))
                 }, 
               # vectorized
               gender_vec = 
                 rep(genders[1:n], 1e6 * gender[1:n]) 
)

gender_vec <- rep(genders[1:n], 1e6 * gender[1:n])
bfi_df$gender <- sample(gender_vec) #assigning proportions to gender vector on bfi_df. adding sample randomizes the order of the vector
```

### Age
```{r}
# let's keep things proportional 
age <- round(table(bfi$age)/nrow(bfi), digits = 5)
age #view age proportions
age_vec <- vector()

n <- length(age)
ages <- as.numeric(names(age)) 
age_vec <- rep(ages[1:n], 1e6 * age[1:n])
bfi_df$age <- sample(age_vec)

```

### Education
```{r}
# let's keep things proportional 
education <- round(table(bfi$education)/nrow(bfi), digits = 5)
education #view education proportions 
education_vec <- c()

n <- length(education)
educations <- as.numeric(names(education)) 
education_vec <- rep(educations[1:n], 1e6 * education[1:n])
bfi_df$education <- sample(education_vec)

```

Every time you manipulate the data frame, check the data before proceeding so that you can see if you've made any mistakes.  Let's see what the final data set looks like. 
```{r}
glimpse(bfi_df) #always check your data after manipulating the data frame before proceeding
```


Earlier, we discovered that RDS and `data.table` functions are the fastest way to save and load data. Saving RDS objects is the most flexible way to write out and read in data without compromising performance, whereas `fwrite()` and `fread` functions were the fastest in terms of raw speed. Let's compare these two approaches using the simulated BFI data. 

```{r}
benchmark(
  saveRDS(bfi_df, "../data/bfi_df.rds"),
  fwrite(bfi_df,"../data/bfi_df_dt.csv"), 
  
  replications = 1,
  columns = c('test', 'elapsed'),
  order = 'elapsed'
)
```

```{r}
benchmark(
  readRDS("../data/bfi_df.rds"),
  fread("../data/bfi_df_dt.csv"), 
  
  replications = 1,
  columns = c('test', 'elapsed'),
  order = 'elapsed'
)
```


## Scoring items 

We can make a list of keys on how to score individual items/variables. Once we obtain item keys, we can score the items on a variety of specifications (e.g., transforming the data on minimum and maximum item values). Run `?psych::scoreItems()` for more information. 

```{r}
keys.list <- list(
  agree = c("-A1", paste0("A", 2:5)),
  conscientious = c(paste0("C",1:3), "-C4", "-C5"),
  extraversion = c("-E1", "-E2", paste0("E", 3:5)),
  neuroticism = paste0("N",1:5), 
  openness = c("O1", "-O2", "O3", "O4", "-O5")
)
bfi_scored <- scoreItems(keys.list, bfi_df[1:25])
bfi_scored

```

## Fast descriptives 

Let's get summary stats and compare performance by common descriptives functions. 

```{r}
pacman::p_load(Hmisc, pastecs, rstatix)

benchmark(
  hmisc = Hmisc::describe(bfi_df),
  psych = psych::describe(bfi_df),
  fpsych = psych::describe(bfi_df, fast=T),
  pastecs = pastecs::stat.desc(bfi_df),
  rstatix = rstatix::get_summary_stats(bfi_df),
  
  replications = 1,
  columns = c('test', 'elapsed'),
  order = 'elapsed'
)

bfi_df_summary <- psych::describe(bfi_df, fast=T)
bfi_df_summary

```
We can see that the `psych` package performs the best. 

Let's get summary stats by group and compare performance by function. We will use the pipeable [`rstatix` package](https://rpkgs.datanovia.com/rstatix/]) that implements commonly used statistical tests with great efficiency. It's easy to use and intuitive if you are familiar with the `tidyverse` framework. 

First let's drop the age column. We only want to work with age and education for now. Let's compare performance between subsetting in base R and using `dplyr::select()`.
```{r}
microbenchmark(unit='s',
               subset = subset(bfi_df, select= -age),
               base = bfi_df[-which(names(bfi_df)=="age")] ,
               dplyr = bfi_df %>% select(-age)
)
```

Indexing using brackets is typically faster than `tidyverse`, but the code is not as accessible. This is a common trade off between the `tidyverse` approach and other approaches. For now, let's use bracket notation to get the data set. Then, let's compare the performance of different functions on group comparisons by gender and education.  

```{r}
bfi_group <- bfi_df[-which(names(bfi_df)=="age")] #or bfi_df %>% select(-age)

benchmark(
  psych_describeBy = psych::describeBy(bfi_group[1:25], list(bfi_group$gender, bfi_group$education), mat=T),
  rstatix_groupby = bfi_group %>% group_by(gender, education) %>% get_summary_stats(), 
  
  replications = 1,
  columns = c('test', 'elapsed'),
  order = 'elapsed'
)

```

Although overall summary statistics using the `psych` package is faster than using `rstatix`, we can see that when obtaining summary statistics after the data are _grouped_, the opposite is true. 

```{r}
bfi_summary_stats_group <- bfi_group %>% group_by(gender, education) %>% get_summary_stats()
bfi_summary_stats_group
```


## Group differences 
An easy way to get standardized mean differences by group is through by `rstatix::cohen_d()`. We can use `pivot_longer()` to gather a column of items and a column of item scores. We can then calculate *d* between groups by item to understand how the items are functioning by group.  

```{r}
bfi_tbl <- as_tibble(bfi_df)  #converting to tbl_df & dropping age

bfi_tbl_group <- bfi_tbl %>%  #grab tbl
  pivot_longer(               #pivot BFI items 
    A1:O5, 
    names_to = "item",        #name gathered column
    values_to = "score"       #name values columns
  ) %>%  
  select(-age, -education) %>% #drop age, edu columns
  group_by(item)              #group by item

glimpse(bfi_tbl_group)

```

This is a large data set, with 25 million rows and 3 variables. Since the data are large, we should make a tinkering tibble to test out different functions for calculating *d*. Let's take the first 1000 rows of our data using `dplyr::slice()`. Since the data are grouped, using `dplyr::slice(1:1000)` would take the first 1000 rows *per item* (i.e., the grouping variable). We need to ungroup, slice, and regroup. 

```{r}
bfi_tbl_group_slice <- 
  bfi_tbl_group %>% 
  ungroup(item) %>% 
  slice(1:1000) %>% 
  group_by(item)

bfi_tbl_group_slice
```

The `rstatix` way is easy to read and follows the grouping command, where in the `psych` way we have to add a group to the formula. Also, the output for each is quite different. We need to code further manipulations using `psych`. 

```{r}
bfi_tbl_group_slice %>% cohens_d(score ~ gender)
```

```{r}
bfi_tbl_group_slice %>%
  cohen.d(score ~ gender + item, data = .) %>% 
  summary
```


Although `psych` might ~2x faster in terms of raw speed, `rstatix` gives us practically useful output that we would have to valuable time coding using `psych`. However, if you only want the effect size without having to program further data manipulations to get a more comprehensive summary, use `psych`. Make sure to double check the direction groups are compared as well. 

```{r}
benchmark( 
  rstatix = bfi_tbl_group_slice %>% 
    cohens_d(score ~ gender),
  
  psych =   bfi_tbl_group_slice %>% 
    cohen.d(score ~ gender + item, data = .) %>% 
    summary ,
  
  replications = 1, 
  columns = c("test", "elapsed"),
  order = "elapsed"
)
```

Yet, given 25 million rows, we would need to calculate 25 effect sizes at a sample of 1 million each. To calculate just 1000 cases took ~.5 seconds using `rstatix`. Scaling up to 25 million would take ~208 minutes. This is a prime example of how inefficiencies can compound when scaling up. 

### Parallelization
A better way to approach the problem is to *parallelize* across all cores. Parallelization is the act of designing a computer program or system to process data in parallel. (You can think of it as hyperthreading across physical cores instead of within cores). Normally, computer programs compute data serially computing one thing at a time. If we parallelize, we can speed up compute by breaking the problem down into smaller pieces to be independently solved simultaneously. 

For this example, we will split the data set into a list, where every element of that list is an item. Then, we can loop through the list using `foreach` and the `%dopar%` operator to run a for loop in parallel. For data sets with smaller cases, we don't see the performance gains. For larger data sets, we do.

```{r}
bfi_tbl_long <-  bfi_tbl_group %>% slice(1:1000) #25k

bfi_ls <- split(
  bfi_tbl_long[, !names(bfi_tbl_long) == "item" ],  #split into list but remove the item variable
  bfi_tbl_long$item #split on item variable
) 

system.time({
  rstatix::cohens_d(bfi_tbl_long, score ~ gender)
})

registerDoParallel(detectCores() - 1) #register the cluster to run in parallel
system.time({
  d_gender <- 
    foreach(i=1:25) %dopar% {
      rstatix::cohens_d(bfi_ls[[i]], score ~ gender) 
    }
})
registerDoSEQ() #register cores to compute sequentially 
```
```{r}
bfi_tbl_long <-  bfi_tbl_group %>% slice(1:1e5)  #2.5 mil (2500k)

bfi_ls <- split(
  bfi_tbl_long[, !names(bfi_tbl_long) == "item" ],  #split into list but remove the item variable
  bfi_tbl_long$item #split on item variable
) 

system.time({
  rstatix::cohens_d(bfi_tbl_long, score ~ gender)
})

registerDoParallel(detectCores() - 1) #register the cluster to run in parallel
system.time({
  d_gender <- 
    foreach(i=1:25) %dopar% {
      rstatix::cohens_d(bfi_ls[[i]], score ~ gender) 
    }
})
registerDoSEQ() #register cores to compute sequentially 
```


To view the results, we can bind the rows across the list if they hold the same column values. There are two ways we can do this: `dplyr::bind_rows()` or `data.table::rbindList()`. 
Both yield the same output, but `rbindList()` is faster. However, it coerces to a `data.table` object. If we want a tibble, we will have to manually transform, which is still faster than `bind_rows()`. 
```{r}
benchmark(
  dplyr::bind_rows(d_gender),
  data.table::rbindlist(d_gender) %>% as_tibble, 
  
  replications = 10, 
  columns = c("test","elapsed"),
  order = "elapsed"
)
```


```{r}
rbindlist(d_gender) %>% as_tibble()
```